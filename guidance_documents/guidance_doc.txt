# ACE: Autonomous Causal Experimentation

## Latest Update: January 21, 2026 - 60% Test Coverage Achieved

**Status:** ‚úÖ Comprehensive test suite implemented - 239 tests, 60% coverage

**Testing Achievement:** 
- 60% coverage (3,666/6,064 statements) - Target: 90%
- 239 tests passing (99% pass rate, ~55s runtime)
- Modern pytest framework following 2026 ML best practices
- Core pipeline, baselines, and visualization fully tested
- See TESTING.md for complete documentation

**Previous Status:** ‚úÖ All improvements implemented and committed, ready for DPO training

**Complete improvements (19 total):**
1. Adaptive diversity threshold - allows strategic 60-75% concentration
2. Value novelty bonus - fixes zero-reward saturation (99% ‚Üí 40-60%)
3. Emergency retraining - prevents gradient death
4. Dynamic candidate reduction - 3-4x faster episodes (13.5m ‚Üí 3-5m)
5. Improved early stopping - triggers at 50-80 episodes
6. PPO bug fix - shape mismatch resolved
7. **Observational training restored** - prevents mechanism forgetting (every 3 steps)
8. Dedicated root learner - active every 3 episodes
9. Paper claims revised - 3 accuracy fixes (lines 609, 661, 714)
10. Verification tools - clamping detector, regime analyzer, extraction scripts

**Launch Commands:**
```bash
./pipeline_test.sh           # Validate (30 min)
sbatch jobs/run_ace_main.sh  # Launch DPO training (4-6 hours)
```

**Key Feature:** Dual observational training (step-level + episode-level) prevents catastrophic forgetting while enabling interventional learning.

**Previous updates:** See CHANGELOG below.

---

## Quick Start

```bash
# Full paper experiments (4-6 hours with improvements, was 12-15h)
./run_all.sh

# Quick validation (~30 minutes)
QUICK=true ./run_all.sh

# Individual components (with improvements)
python ace_experiments.py --episodes 200 --early_stopping --root_fitting
python baselines.py --all_with_ppo --episodes 100
python visualize.py results/run_*/
```

---

## Project Overview

ACE is a system for learning Structural Causal Models (SCMs) through autonomous experimentation. An agent policy proposes interventions (DO operations), executes them in a ground truth environment, and trains a student SCM to match the observed mechanisms.

### The Core Challenge: Collider Resolution

The ground truth SCM contains a **collider structure** that is particularly difficult to learn:

```
     X1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí X3 (COLLIDER)
      ‚îÇ           ‚îÇ             ‚Üë
      ‚îî‚îÄ‚îÄ‚îÄ‚Üí X2 ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      
     X4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí X5
```

**Ground Truth Mechanisms:**
- X1 ~ N(0, 1)  [exogenous root]
- X4 ~ N(2, 1)  [exogenous root]
- X2 = 2.0 * X1 + 1.0 + Œµ
- X3 = 0.5 * X1 - X2 + sin(X2) + Œµ  **[COLLIDER - 2 parents]**
- X5 = 0.2 * X4¬≤ + Œµ

**Why X3 is Hard:**
1. X3 has TWO parents (X1 and X2) - a collider
2. X1 and X2 are highly correlated: X2 = 2*X1 + 1
3. Under DO(X1=v), X2 still follows X2=2v+1, so they remain collinear
4. Only DO(X2=v) breaks this correlation, allowing independent variation
5. Without balanced interventions on BOTH X1 and X2, the learner cannot disentangle their effects

---

## Current Status (2026-01-21, 10:15 AM)

### Ready to Launch DPO Training ‚úÖ

**All improvements committed (7 git commits):**
- ‚úÖ Training fixes: Adaptive diversity, novelty bonus, emergency retrain, speedups
- ‚úÖ Observational training: RESTORED (step-level + dedicated root learner)
- ‚úÖ PPO bug: FIXED (shape mismatch resolved)
- ‚úÖ Paper revisions: 3 claims corrected for accuracy
- ‚úÖ Verification tools: clamping detector, regime analyzer, extraction scripts
- ‚úÖ Git: Clean, ready to push

**Experimental Status:**
- ‚úÖ Baselines: Complete (Round-Robin 1.99 best, Random 2.17, Max-Var 2.09)
- ‚úÖ Root learner: Validated (98.7% improvement)
- ‚úÖ Duffing & Phillips: Complete
- üîÑ Complex SCM: Running (greedy_collider)
- ‚è≥ ACE main: Ready to launch with all fixes

**Launch Commands:**
```bash
./pipeline_test.sh           # Test fixes (30 min)
sbatch jobs/run_ace_main.sh  # ACE training (4-6 hours)
```

**What Will Run:**
- 200 episodes (early stop expected at 40-80)
- Observational training every 3 steps (200 samples)
- Dedicated root learner every 3 episodes (1000 samples)
- All Jan 21 fixes active
- Expected runtime: 4-6 hours (vs previous 30-40h)

**After Training:**
```bash
./verify_claims.sh           # Verify all paper claims
./extract_ace.sh             # Extract metrics for tables
python compare_methods.py    # Generate Table 1
```

### Paper Readiness
- ‚úÖ All 5 experiments analyzed and aligned
- ‚úÖ 3 paper claims revised for accuracy (lines 609, 661, 714)
- ‚è≥ Need ACE results to fill Table 1
- ‚è≥ Need verification of behavioral claims
- üìÖ ETA to submission: 3-5 days after launch

### Paper Experiments Status
| Experiment | Paper Section | Status |
|------------|---------------|--------|
| Synthetic 5-node SCM | 3.4.1 | ‚úÖ `ace_experiments.py` |
| Complex 15-node SCM | 3.4.2 (NEW) | ‚úÖ `experiments/complex_scm.py` |
| Baselines (Random, Round-Robin, Max-Variance) | 3.8 | ‚úÖ `baselines.py` |
| PPO Comparison | Discussion | ‚úÖ `baselines.py` |
| Coupled Duffing Oscillators | 3.6 | ‚úÖ `experiments/duffing_oscillators.py` |
| US Phillips Curve (FRED) | 3.7 | ‚úÖ `experiments/phillips_curve.py` |

### Historical Context
Analysis of `run_20260103_141404` revealed original failure:
- LLM generated X1 for 100% of candidates
- DPO stuck at random chance (loss = 0.693)
- This was fixed via supervised pre-training and smart collapse breaker

---

## Current Solution Approach

### 1. Problem-First Prompt Restructuring
Old prompt buried loss info and ended with "DO" ‚Üí biased toward X1.
New prompt:
- STARTS with "PROBLEM: Node losses - X3=1.82, X2=0.01..."
- Explicitly says "To fix X3, intervene on parents X1, X2"
- Warns "X1 is over-sampled. AVOID X1."
- Includes few-shot reasoning examples
- Ends with "The highest loss node is" to prompt reasoning

### 2. Supervised Pre-training Before DPO
- `supervised_pretrain_llm()` runs 100 steps BEFORE DPO
- Trains on teacher-generated interventions (loss-aware, balanced)
- Gives LLM proper starting point instead of random X1 fixation
- Periodic re-training every 50 episodes to combat drift

### 3. Smart Collapse Breaker
- When all candidates target collapsed node and collapse detected:
- PRIORITIZE parents of high-loss colliders (X2 for failing X3)
- Don't randomly inject X3/X4/X5 which don't help

### 4. Comprehensive DPO Logging
New `DPOLogger` class tracks:
- Loss values and trends
- Preference margins (policy_win - policy_lose)
- Sigmoid inputs (positive = learning, near-zero = stuck)
- KL divergence from reference policy
- Winner/loser target distribution
- Periodic health check reports with warnings

### 5. Gradient Monitoring
- Logs gradient norms every 20 episodes
- Warns if near-zero (DPO not training)

---

## Output Files Per Run

Each run generates:
1. `scm_graph.png` - **NEW** Visualization of the SCM structure with node types
2. `mechanism_contrast.png` - Learned vs ground truth mechanisms
3. `training_curves.png` - DPO loss and reward over time
4. `strategy_analysis.png` - Target preference and value distribution
5. `metrics.csv` - Per-step metrics (target, value, reward, score)
6. `node_losses.csv` - Per-step per-node mechanism losses
7. `dpo_training.csv` - **NEW** DPO training diagnostics
8. `experiment.log` - Full execution log

---

## Results Documentation Protocol

**CRITICAL:** Whenever an experimental result supports a paper claim, document it immediately in `results/RESULTS_LOG.md`.

### Why Document Results?

1. **Audit Trail:** Clear path from raw data ‚Üí evidence ‚Üí paper claims
2. **Prevent Loss:** Don't lose important findings in log files
3. **Track Claims:** Know which claims are supported vs aspirational
4. **Review Readiness:** Easy to verify all claims before submission

### What to Document

Document EVERY experimental run that:
- ‚úÖ Supports a paper claim
- ‚ö†Ô∏è Partially supports a paper claim
- ‚ùå Contradicts a paper claim
- üî¨ Tests a specific hypothesis

### When to Document

**IMMEDIATELY** after:
- Completing a full experimental run
- Extracting metrics from logs
- Identifying a key finding
- Discovering an unexpected result

### How to Document

1. **Add entry to `results/RESULTS_LOG.md`** with:
   - Date and run ID
   - Key metrics and findings
   - Which paper lines/claims it supports
   - Path to evidence files
   - Any caveats or issues

2. **For major findings, create `results/claim_evidence/[claim].md`**
   - Detailed evidence for specific claim
   - Multiple runs supporting same claim
   - Statistical analysis if available

3. **Save publication-ready figures to `results/figures/`**
   - Include metadata about source run
   - Use consistent naming convention

### Example Workflow

```bash
# 1. Run experiment
sbatch jobs/run_ace_main.sh

# 2. Wait for completion
tail -f logs/ace_*.out

# 3. Extract key metrics
grep "Final mechanism losses:" results/paper_*/ace/experiment.log

# 4. IMMEDIATELY document in results/RESULTS_LOG.md
code results/RESULTS_LOG.md
# Add new entry with date, metrics, paper claims supported

# 5. If this supports a paper table, note it
# Example: "Fills Table 1, line 428-437"

# 6. Generate figures if needed
python visualize.py results/paper_*/ace/
cp results/paper_*/ace/learning_curves.png results/figures/

# 7. Link to paper
# In RESULTS_LOG.md, note: "Supports line 439: superior collider identification"
```

### Before Writing Paper Claims

**DO:**
- ‚úÖ Check `results/RESULTS_LOG.md` for supporting evidence
- ‚úÖ Use actual numbers from documented runs
- ‚úÖ Reference specific evidence files
- ‚úÖ Note any caveats or limitations

**DON'T:**
- ‚ùå Write claims before having results
- ‚ùå Use placeholder values
- ‚ùå Make aspirational claims without marking them
- ‚ùå Forget to document negative results

### Before Paper Submission

**Checklist:**
- [ ] Every paper claim has entry in `results/RESULTS_LOG.md`
- [ ] All tables filled with real numbers (no placeholders)
- [ ] All figures generated and saved to `results/figures/`
- [ ] Evidence files accessible and organized
- [ ] Summary dashboard in RESULTS_LOG.md up to date

See `results/README.md` for detailed templates and examples.

---

## Testing Infrastructure

### Comprehensive Test Coverage (January 21, 2026)

**Status:** 60% coverage achieved, continuing toward 90% goal  
**Tests:** 239 passing (99% pass rate)  
**Documentation:** See `TESTING.md` for complete guide  
**Framework:** Modern pytest with statistical and property-based testing

#### Testing Strategy

**Test Categories:**
1. **Unit Tests** - Fast (<100ms), isolated component tests
2. **Integration Tests** - Multi-component workflow tests
3. **Statistical Tests** - Probabilistic assertions with tolerances
4. **Regression Tests** - Performance baseline tracking
5. **Property Tests** - Hypothesis-based invariant testing

**Coverage Targets:**
- Core SCM classes: 95%
- Policy implementations: 90%
- Baseline policies: 90%
- Training loops: 85%
- Experiments: 80%
- Visualization: 70%

#### Key Testing Principles

1. **Behavior over Implementation**: Test external expectations, not internal details
2. **Statistical Assertions**: Use `pytest.approx` for probabilistic components
3. **Reproducibility**: All tests use fixed seeds (`seed_everything` fixture)
4. **Fast Feedback**: Unit tests complete in <1s, integration in <30s
5. **Regression Prevention**: Track baselines for all key metrics

#### Test Framework

**Dependencies:**
```bash
pytest>=8.0.0
pytest-cov>=5.0.0
pytest-mock>=3.12.0
pytest-timeout>=2.2.0
pytest-xdist>=3.5.0      # Parallel execution
hypothesis>=6.100.0      # Property-based testing
```

**Running Tests:**
```bash
# All unit tests (fast)
pytest -m unit

# With coverage
pytest -m unit --cov=. --cov-report=html

# Integration tests
pytest -m integration

# All except slow
pytest -m "not slow"

# Parallel execution
pytest -n auto
```

#### Test Structure

```
tests/
‚îú‚îÄ‚îÄ conftest.py              # Shared fixtures
‚îú‚îÄ‚îÄ pytest.ini               # Configuration
‚îú‚îÄ‚îÄ test_ground_truth_scm.py # Core SCM tests
‚îú‚îÄ‚îÄ test_student_scm.py      # Student model tests
‚îú‚îÄ‚îÄ test_llm_policy.py       # LLM policy tests
‚îú‚îÄ‚îÄ test_dpo_trainer.py      # DPO training tests
‚îú‚îÄ‚îÄ test_reward_functions.py # Reward computation tests
‚îú‚îÄ‚îÄ baselines/               # Baseline policy tests
‚îú‚îÄ‚îÄ integration/             # E2E workflow tests
‚îú‚îÄ‚îÄ statistical/             # Property tests
‚îú‚îÄ‚îÄ regression/              # Performance tracking
‚îî‚îÄ‚îÄ experiments/             # Experiment-specific tests
```

#### Key Fixtures

```python
@pytest.fixture
def seed_everything():
    """Set all random seeds for reproducibility."""
    
@pytest.fixture
def ground_truth_scm():
    """GroundTruthSCM instance."""
    
@pytest.fixture
def student_scm():
    """Untrained StudentSCM instance."""
    
@pytest.fixture
def mock_llm():
    """Mock LLM for GPU-free testing."""
    
@pytest.fixture
def test_output_dir():
    """Temporary directory for test outputs."""
```

#### Test Markers

```python
@pytest.mark.unit           # Fast, isolated tests
@pytest.mark.integration    # Multi-component tests
@pytest.mark.slow           # End-to-end tests (>30s)
@pytest.mark.statistical    # Probabilistic assertions
@pytest.mark.regression     # Performance tracking
@pytest.mark.requires_gpu   # GPU-dependent tests
@pytest.mark.requires_hf    # HuggingFace models required
```

#### Example Test

```python
@pytest.mark.unit
@pytest.mark.statistical
def test_x3_collider_mechanism(scm, seed_everything):
    """Test X3 = 0.5*X1 - X2 + sin(X2) mechanism."""
    seed_everything(42)
    n = 10000
    
    # Generate observational data
    data = scm.generate(n)
    
    # Compute expected X3
    expected = 0.5 * data['X1'] - data['X2'] + torch.sin(data['X2'])
    
    # Assert match (allowing for noise std=0.1)
    actual = data['X3']
    residuals = (actual - expected).abs()
    
    # Should be centered near 0 with std ‚âà 0.1
    assert residuals.mean() == pytest.approx(0.0, abs=0.05)
    assert residuals.std() == pytest.approx(0.1, abs=0.05)
```

#### CI/CD Integration

**GitHub Actions** (`.github/workflows/test.yml`):
- Run on every push/PR
- Test on Python 3.9, 3.10, 3.11
- Upload coverage to Codecov
- Block merge if coverage drops

**Pre-commit Hooks**:
- Run unit tests before commit
- Ensure code quality

#### Implementation Timeline

| Week | Phase | Deliverables |
|------|-------|-------------|
| 1 | Infrastructure + Core | Framework, SCM tests |
| 2-3 | Policy Components | LLM, DPO, reward tests |
| 4 | Integration | E2E workflow tests |
| 5 | Statistical + Regression | Property, baseline tests |
| 6 | Experiments | Domain-specific tests |
| 7 | CI/CD | GitHub Actions, docs |

**Total Effort:** 7 weeks (1 developer full-time)

#### Maintenance

**Ongoing:**
- Update tests with code changes
- Update regression baselines after verified improvements
- Monitor coverage in CI/CD
- Hunt and fix flaky tests
- Track test suite performance

**Quarterly:**
- Review coverage reports
- Identify undertested components
- Update strategy based on bug patterns
- Refactor for maintainability

#### Success Metrics

| Milestone | Unit Cov | Integration Cov | Overall Cov |
|-----------|----------|----------------|-------------|
| Week 2    | 50%      | 0%             | 40%         |
| Week 4    | 80%      | 50%            | 70%         |
| Week 6    | 90%      | 80%            | 85%         |
| Complete  | 95%      | 90%            | 90%+        |

**Quality Targets:**
- All tests pass on Python 3.9-3.11
- Test suite completes in <5 minutes
- >99% pass rate (no flaky tests)
- Critical components >90% coverage

---

## Project Scripts

### Main Experiment: `ace_experiments.py`
The core ACE framework with LLM-based policy and DPO training.

### Baselines: `baselines.py`
Implements four baseline intervention strategies for comparison:

1. **Random Policy (Lower Bound)**
   - Uniformly samples target node and intervention value
   - Represents unguided, passive exploration

2. **Round-Robin (Systematic Heuristic)**
   - Cycles through nodes in fixed topological order
   - Tests if adaptive, non-uniform sampling is necessary

3. **Max-Variance (Uncertainty Sampling)**
   - Greedy active learning using MC Dropout
   - Selects interventions maximizing predictive variance
   - Represents standard "greedy" optimal experimental design

4. **PPO (Proximal Policy Optimization)**
   - Actor-Critic RL baseline for fair comparison with DPO
   - Same reward signal: Information Gain + coverage bonuses
   - Same replay buffer, observational training interval
   - Uses GAE for advantage estimation, clipped surrogate objective
   - Tests paper's claim that DPO outperforms value-based RL

```bash
# Run individual baseline
python baselines.py --baseline random --episodes 100
python baselines.py --baseline round_robin --episodes 100
python baselines.py --baseline max_variance --episodes 100
python baselines.py --baseline ppo --episodes 100

# Run all baselines (excluding PPO - faster)
python baselines.py --all --episodes 100

# Run all baselines including PPO
python baselines.py --all_with_ppo --episodes 100
```

### Visualization: `visualize.py`
```bash
python visualize.py run_YYYYMMDD_HHMMSS/  # Success verification dashboard
```

### Additional Experiments: `experiments/`
```bash
# Complex 15-Node SCM (Hard benchmark to show strategy advantages)
python -m experiments.complex_scm --policy random --episodes 200
python -m experiments.complex_scm --policy smart_random --episodes 200
python -m experiments.complex_scm --policy greedy_collider --episodes 200

# Duffing Oscillators (Physics - ODE-based causal discovery)
python -m experiments.duffing_oscillators --episodes 100

# Phillips Curve (Economics - FRED data retrospective learning)
python -m experiments.phillips_curve --episodes 50
```

**Why Complex SCM?**
The simple 5-node SCM proved too easy - even random sampling learns all mechanisms.
The 15-node SCM is designed to be HARD:
- 15 nodes (3x larger)
- 5 colliders including nested collider (N1 depends on L1 which is itself a collider)
- 5 hierarchical layers (vs 2)
- Mix of linear, polynomial, trig, interaction terms
- Higher noise (0.2 vs 0.1)
- Random sampling spreads samples too thin across 15 nodes

This should reveal advantages of strategic intervention policies.

### Full Pipeline: `run_all.sh` (HPC/SLURM)
```bash
sbatch run_all.sh                           # Full paper experiments
QUICK=true sbatch run_all.sh                # Quick validation
ACE_EPISODES=200 sbatch run_all.sh          # Custom episodes
squeue -u $USER                             # Monitor job
tail -f logs/paper_*.out                    # View progress
```

---

## Key CLI Arguments

### ACE Main Script
```bash
python ace_experiments.py \
    --episodes 100 \
    --pretrain_steps 100 \        # Supervised pre-training before DPO
    --pretrain_interval 50 \      # Re-train every N episodes
    --smart_breaker \             # Prioritize collider parents in collapse breaker
    --obs_train_interval 5 \      # Observational training every N steps (0=disabled)
    --obs_train_samples 100 \     # Samples per observational injection
    --obs_train_epochs 50 \       # Training epochs for observational data
    --debug_parsing               # Enable parse debug logging
```

### Baselines Script
```bash
python baselines.py \
    --baseline random \           # Or: round_robin, max_variance, ppo
    --all \                       # Run all baselines (excluding PPO)
    --all_with_ppo \              # Run all baselines including PPO
    --episodes 50 \               # Number of episodes
    --steps 25 \                  # Steps per episode
    --obs_train_interval 5 \      # Observational training interval
    --ppo_lr 3e-4 \               # PPO learning rate
    --ppo_epochs 4 \              # PPO epochs per update
    --ppo_clip 0.2 \              # PPO clip epsilon
    --output results              # Output directory
```

### Run All Script (Paper Experiments)
```bash
python run_all.py \
    --quick \                     # Fast validation (~5 min)
    --full \                      # Full paper experiments (~2-4 hours)
    --ace_episodes 500 \          # Override ACE episodes
    --baseline_episodes 100 \     # Override baseline episodes
    --use_custom \                # Use custom transformer (faster)
    --skip_ppo \                  # Skip PPO baseline
    --timeout 14400               # Timeout per experiment (seconds)
```

---

## Success Criteria

### Primary (All mechanisms learned)
1. **X3 Loss < 0.5** - The collider mechanism is learned ‚úÖ (achieved ~0.07)
2. **X2 Loss < 1.0** - The linear mechanism is preserved (requires obs training)
3. **X5 Loss < 0.5** - The quadratic mechanism is learned ‚úÖ (achieved ~0.18)
4. **X1, X4 Loss < 1.0** - Root distributions captured

### Secondary (Training health)
5. **DPO Learning** - Loss should decrease from 0.693 toward 0
6. **Preference Margin Positive** - Policy should prefer winners >50% of time
7. **Intervention Diversity** - No single node >70% of interventions

### Paper Validation
8. **ACE > Random** - Lower final loss than random baseline
9. **ACE > Round-Robin** - Lower final loss than systematic baseline
10. **ACE > Max-Variance** - Lower final loss than greedy active learning
11. **ACE > PPO** - Lower final loss than value-based RL (validates paper claim)

---

## Debugging Checklist

### If X3 (collider) isn't learning:
1. Check `metrics.csv` - Is X2 getting interventions?
2. Check `value_diversity.csv` - Are X2 values diverse (not stuck at single value)?
3. Check `dpo_training.csv` - Is preference_margin positive?
4. Check `mechanism_contrast.png` - Are X2‚ÜíX3 and X1‚ÜíX3 slices accurate?

### If X2 mechanism is forgotten (high loss):
1. Verify `--obs_train_interval` is enabled (default: 3)
2. Check intervention distribution - if >90% X2, increase observational training
3. Check `mechanism_contrast.png` - Is X1‚ÜíX2 line flat (untrained network)?

### If root nodes (X1, X4) aren't learning:
1. Enable `--root_fitting` for explicit root distribution fitting
2. Increase observational training: `--obs_train_interval 2`
3. Check that root_fit logs appear every 5 episodes
4. Verify interventions aren't overshadowing observational data

### If DPO isn't learning:
1. Check gradient norms in log - Are they non-zero?
2. Check `dpo_training.csv` - Is loss stuck at 0.693?
3. Verify supervised pre-training ran (`--pretrain_steps 100`)

### General:
4. Check `scm_graph.png` - Verify graph structure is correct
5. Run `python visualize.py <run_dir>/` for comprehensive dashboard

---

# CHANGELOG

## 2026-01-15: Periodic Observational Training to Prevent Mechanism Forgetting

### Problem: Catastrophic Forgetting of X2 Mechanism
Analysis of `run_20260113_060752` and `run_20260114_095334` revealed:
- X3 collider was learned successfully (loss ~0.07) ‚úì
- X2 mechanism was **catastrophically forgotten** (loss ~22) ‚úó
- Intervention distribution was 98.9% X2, only 1.1% X1

**Root Cause:**
When `DO(X2=v)` is applied, X2 is set directly to value `v`. This means:
1. The student's X2 mechanism `f(X1) ‚Üí X2` is **never called**
2. The X1‚ÜíX2 relationship `X2 = 2¬∑X1 + 1` receives no gradient updates
3. After 1500+ X2 interventions, the X2 mechanism network outputs garbage

### Solution: Periodic Observational Training
Every N steps, inject observational data (no intervention) to preserve all mechanisms:

```python
# New CLI arguments:
--obs_train_interval 5    # Train on observational data every 5 steps
--obs_train_samples 100   # Number of samples per injection
--obs_train_epochs 50     # Training epochs for observational data
```

**Implementation:**
- After each `learner.train_step(intervention_data)`, check if `step % obs_train_interval == 0`
- If so, generate observational samples: `M_star.generate(n_samples, interventions=None)`
- Train the student on this data, which includes the natural X2 mechanism

### Expected Outcome
- X2 mechanism should now be preserved (loss < 1.0)
- X3 collider should still be learned (loss < 0.5)
- Both mechanisms coexist without catastrophic forgetting

---

## 2026-01-13: Value-Aware Collapse Breaking for Collider Disentanglement

### Problem: Single-Value Trap (X2=c)
Analysis of `run_20260112_011254` revealed a new failure mode:
- The agent correctly shifted from X1 to X2.
- However, it collapsed to a **single static value** `DO(X2 = 1.5)`.
- **Consequence:** 
    - The learner observed a 1D slice of the X3 mechanism ($X3 = f(X1, 1.5)$).
    - This slice is easily fit by a linear model, resulting in **Zero Training Loss**.
    - The validation set (where X2 varies) showed **High Error**, but the reward signal (delta loss) was zero because the agent kept repeating the same "perfect" action.
    - The standard Collapse Breaker failed because it injected X1 (correlated) or X4 (irrelevant), never forcing X2 to vary.

### Solution: Value-Aware Collapse Breaker
We implemented a principled extension to the Smart Breaker:
1.  **Context Detection:** Identifies if the collapsed node is a **Parent of a Collider** (e.g., X2 is parent of X3).
2.  **Constraint:** Recognizes that for collider parents, *existence* of intervention isn't enough; *variance* is required.
3.  **Action:** Injects an intervention for the **SAME NODE** (X2) but with a **RADICALLY DIFFERENT VALUE** (e.g., flipping sign or moving to bounds).
4.  **Mechanism:**
    - Calculates mean of recent values (e.g., 1.5).
    - Samples `new_val` from a disjoint range (e.g., [-5, 0]).
    - This forces the learner to predict on a new support region, exposing the error in its simplified model and generating a strong gradient signal.

### Expected Outcome
- The logs should show `[Smart Breaker] Injected 'DO X2 = -3.5' (value_diversity_for_collider_parent...)`.
- X3 Loss should drop significantly as the learner sees X2 vary independently of X1.
- DPO should pick up this signal (via the "Epistemic Boost") and learn to vary X2 autonomously.

---

## 2026-01-11: Comprehensive Logging and SCM Visualization

### Changes
1. **SCM Graph Visualization** (`visualize_scm_graph`)
   - Generates `scm_graph.png` for each run
   - Shows node types: root (green), intermediate (blue), collider (red), leaf (purple)
   - Displays mechanism equations and final losses
   - Uses hierarchical layout for clear causal structure

2. **DPO Training Logger** (`DPOLogger`)
   - Tracks loss, preference margins, sigmoid inputs, KL divergence
   - Periodic health reports every 100 steps
   - Warns when DPO appears stuck (loss ~0.693, negative preference margin)
   - Saves to `dpo_training.csv` for post-hoc analysis

3. **Enhanced DPO Loss Function**
   - Now logs all components for diagnosis
   - Extracts winner/loser targets for tracking

---

## 2026-01-11: LLM Policy Attention Failure - Direct Intervention

### Problem: LLM Completely Ignores Prompt
- Total Policy Collapse: 100% of candidates targeted X1
- DPO stuck at 0.693 (random chance)
- X3 loss remained at 1.8-2.5 while X2 = 0.01

### Solution
1. Problem-first prompt restructuring
2. Supervised pre-training phase (100 steps before DPO)
3. Smart collapse breaker (prioritizes collider parents)
4. Gradient monitoring every 20 episodes
5. LLM fallback on parse failure

---

## 2026-01-08: Policy Input Augmentation for Collider Resolution

### Problem
Agent was "blind" - only saw weights, not losses. Couldn't learn "if X3 failing, intervene on X2".

### Solution
1. Augmented StateEncoder with per-node validation loss
2. LLM prompt includes loss values
3. Reward clipping to -2.0 (reduced punishment for exploration)

---

## 2026-01-03: Fast Adaptation Phase

### Problem
Reward misattribution due to delayed learning updates.

### Solution
Added fast adaptation phase that trains on NEW data before replay consolidation.

---

## 2026-01-20: Comprehensive Training Efficiency Overhaul

### Problems Identified from January 19, 2026 HPC Run Analysis
Based on detailed artifact analysis of `run_20260119_123852`:
1. **Training Saturation:** 89.3% of steps (3,358/3,760) produced zero reward
2. **Root Node Failure:** X1 showed NO learning (0.879‚Üí0.879), X4 got worse (1.506‚Üí1.564)
3. **Policy Collapse:** LLM generated X2 for 99.1% of candidates
4. **KL Divergence Explosion:** Reference policy diverged from 0 ‚Üí -2,300

### Solutions Implemented

#### 1. Early Stopping System
**Class:** `EarlyStopping` (lines 1413-1467)
- Detects loss plateaus (no improvement for N episodes)
- Detects training saturation (>85% zero-reward steps)
- **Impact:** Reduces runtime from 9h 11m to 1-2h (80% savings)

**New Arguments:**
```bash
--early_stopping              # Enable early stopping
--early_stop_patience 20      # Episodes to wait before stopping
--early_stop_min_delta 0.01   # Minimum improvement threshold
--zero_reward_threshold 0.85  # Stop if 85% steps have zero reward
```

#### 2. Root Node Learning Enhancements
**Problem:** Interventions DO(X1=v) override natural distribution N(0,1)
**Solution:** 3x increase in observational training + explicit root fitting

**Updated Defaults:**
```bash
--obs_train_interval 3        # Was 5 (67% more frequent)
--obs_train_samples 200       # Was 100 (2x samples)
--obs_train_epochs 100        # Was 50 (2x epochs)
```

**New Function:** `fit_root_distributions()` (lines 1470-1547)
- Explicitly fits root distributions using pure observational data
- Trains mu and sigma parameters directly
- Called every 5 episodes

**New Arguments:**
```bash
--root_fitting                # Enable root-specific fitting
--root_fit_interval 5         # Fit every N episodes
--root_fit_samples 500        # Samples for root fitting
--root_fit_epochs 100         # Epochs for root fitting
```

**Expected Impact:**
- X1 loss: 0.879 ‚Üí <0.3 (66% reduction)
- X4 loss: 0.942 ‚Üí <0.3 (68% reduction)

#### 3. Multi-Objective Diversity Rewards
**Problem:** Policy collapsed to 69.4% X2, requiring constant hard-cap enforcement
**Solution:** Smooth diversity penalties + coverage bonuses

**New Functions:**
- `compute_diversity_penalty()` (lines 1549-1578): Penalizes concentration >50%
- `compute_coverage_bonus()` (lines 1581-1601): Rewards exploring all nodes

**Updated Defaults:**
```bash
--undersampled_bonus 200.0    # Was 100.0 (2x stronger)
```

**New Arguments:**
```bash
--diversity_reward_weight 0.3     # Weight for diversity in multi-objective reward
--max_concentration 0.5           # Maximum 50% on any single node
--concentration_penalty 200.0     # Penalty weight for exceeding max
```

**Reward Formula (Updated):**
```python
base_score = reward + cov_bonus + ... - leaf_pen - collapse_pen
diversity_score = diversity_penalty + coverage_bonus
final_score = base_score + diversity_weight * diversity_score
```

**Expected Impact:**
- X2 concentration: 69.4% ‚Üí <50%
- X4 interventions: 0.9% ‚Üí >15% (16x increase)
- X5 interventions: 0.0% ‚Üí >10%

#### 4. Reference Policy Stability
**Problem:** KL divergence from 0 ‚Üí -2,300 (policy abandoned supervised initialization)
**Solution:** Periodic reference policy updates

**New Arguments:**
```bash
--update_reference_interval 25    # Update reference every N episodes
```

**Implementation:** Updates reference policy independently of supervised re-training,
keeping KL divergence bounded and maintaining connection to initialization.

### Usage

**Recommended Command (All Improvements):**
```bash
python ace_experiments.py \
  --episodes 200 \
  --early_stopping \
  --early_stop_patience 20 \
  --obs_train_interval 3 \
  --obs_train_samples 200 \
  --obs_train_epochs 100 \
  --root_fitting \
  --root_fit_interval 5 \
  --undersampled_bonus 200.0 \
  --diversity_reward_weight 0.3 \
  --max_concentration 0.5 \
  --update_reference_interval 25 \
  --pretrain_steps 200 \
  --pretrain_interval 25 \
  --smart_breaker \
  --output results/ace_improved
```

**Quick Test (10 episodes):**
```bash
python ace_experiments.py \
  --episodes 10 \
  --early_stopping \
  --root_fitting \
  --output results/test
```

### Expected Overall Performance

| Metric | Before (Jan 19) | After (Jan 20) | Improvement |
|--------|-----------------|----------------|-------------|
| Runtime | 9h 11m | 1-2h | 80% ‚Üì |
| X1 Loss | 0.879 | <0.3 | 66% ‚Üì |
| X4 Loss | 0.942 | <0.3 | 68% ‚Üì |
| X2 Concentration | 69.4% | <50% | 28% ‚Üì |
| Zero-reward Steps | 89.3% | <50% | 44% ‚Üì |
| Total Loss | 1.92 | <1.0 | 48% ‚Üì |

### Backwards Compatibility
All improvements are opt-in via flags. Existing scripts work unchanged.
New defaults (obs_train_interval=3, undersampled_bonus=200) improve performance automatically.
