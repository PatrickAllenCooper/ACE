# ACE: Autonomous Causal Experimentation

## Latest Update: January 26, 2026 - Repository Reorganization & ICML Reviewer Response

**Major Changes:**
- **Repository Reorganization:** Scripts organized into logical directories
  - `scripts/analysis/` - Analysis tools (visualize, compare, verify)
  - `scripts/maintenance/` - Cleanup and maintenance scripts
  - `scripts/runners/` - Local execution wrappers
  - `jobs/workflows/` - Multi-seed and ablation orchestration
  - `results/logs/` - Consolidated job logs
  - `results/summaries/` - High-level result summaries
  - `results/archive/` - Archived documentation

**Status:** ABLATIONS FAILED & FIXED - Critical experiments queued

**ABLATION CATASTROPHIC FAILURE (Jan 26, 6 PM):**
- All 20+ ablation jobs failed silently (100+ GPU-hours wasted)
- Root cause: Import bug in run_ablations_scratch.sh (cd to scratch before Python)
- Zero output files produced despite jobs completing
- Bug fixed, logic implemented, comprehensive tests added, verified locally
- Ready to re-run with streamlined fast ablations (3-5h total)

**Reviewer Response Changes Made:**
- Fixed beta notation overload (reward weight gamma vs DPO temperature beta_DPO)
- Added explicit hyperparameter values (alpha=0.1, gamma=0.05, beta_DPO=0.1)
- Added LM policy justification (variable graphs, pretrained biases, cost balance)
- Fixed statistical test inconsistency (paired t-tests, Bonferroni correction)
- Softened p-value claims (3/4 significant, Max-Var marginal)
- Clarified Phillips curve as "active data subset selection" not causal intervention
- Expanded Limitations section with 5 dedicated paragraphs on known issues
- Removed topology recovery claim from Duffing section

**POTENTIAL REVIEWER CONCERNS (Address in writing if no time for experiments):**

1. **Oracle pretraining** - 200 interventions from oracle is substantial privileged info.
   - Even if ablated, it's in main results. Reviewers may object.
   - WRITING FIX: Emphasize in Limitations that this warm-start accelerates but isn't necessary; ablation quantifies contribution. Could add sentence: "We view oracle pretraining as analogous to expert demonstrations in imitation learning; future work will explore curriculum alternatives."

2. **N=5 seeds** - Thin by ML standards. One outlier (seed 789) drives high variance.
   - N=10 would be more robust but may not have time.
   - WRITING FIX: Add to Limitations: "With N=5 seeds, statistical power is limited; we report medians for robustness to the observed outlier and recommend N>=10 for future studies."

3. **Phillips curve framing** - Acknowledged as "active data subset selection" not causal intervention.
   - May still read as weak or overreaching.
   - WRITING FIX: Consider downplaying in abstract/intro, framing as "demonstration of strategic historical sampling" rather than causal experiment. Possibly move to appendix if space allows.

4. **LM policy overhead** - Qwen2.5-1.5B may seem like overkill.
   - Reviewers may ask "why not a simple MLP policy?"
   - WRITING FIX: Strengthen justification already in paper (variable graphs, pretrained biases, cost balance). Add: "We chose an LM over MLP/GNN because (1) text prompts naturally handle variable-size graphs without architectural changes, (2) pretrained representations transfer useful inductive biases, and (3) 1.5B is smallest model with robust instruction-following. An MLP baseline would require fixed graph encoding, limiting generalization."

**RECOVERY PLAN - Ablations:**

1. **On HPC, pull fixed code:**
   ```bash
   cd ~/ACE
   git pull
   python scripts/runners/verify_ablations.py  # Should show all PASS
   ```

2. **Submit fast ablations (OPTIONAL - Nice to have but not critical):**
   ```bash
   sbatch jobs/run_ablations_fast.sh
   # Runtime: 3-5 hours (4 ablations × 3 seeds, early stopping)
   ```

3. **Or skip ablations and focus on critical experiments** (recommended if time-constrained)

**CRITICAL EXPERIMENTS TO RUN (PRIORITY):**

1. **Extended Baselines (171 episodes)** - ADDRESSES: unfair budget comparison
   ```bash
   python scripts/runners/run_critical_experiments.py --extended-baselines
   ```

2. **Lookahead Ablation (Random Proposer)** - ADDRESSES: lookahead confound
   ```bash
   python scripts/runners/run_critical_experiments.py --lookahead-ablation
   ```

3. **Complex 15-Node SCM (Full)** - ADDRESSES: scaling validation
   ```bash
   python scripts/runners/run_critical_experiments.py --complex-scm
   ```

4. **Run ALL critical experiments:**
   ```bash
   sbatch jobs/run_critical_experiments.sh
   # OR locally:
   python scripts/runners/run_critical_experiments.py --all --seeds 42 123 456 789 1011
   ```

**Current Experimental Status:**
- ACE: COMPLETE (N=5 seeds)
  - Median: 0.61 total loss (mean: 0.92 +/- 0.73)
  - 52-58% improvement over baselines (3/4 significant)
  - Strategic concentration: 99.8% on collider parents
- Baselines (100 episodes): COMPLETE - NEED 171-EPISODE EXTENSION
- Duffing: COMPLETE (N=5, loss 0.042 +/- 0.036)
- Phillips: COMPLETE (N=5)
- Complex SCM: PARTIAL - NEED FULL COMPARISON

**Paper Status:** 90% complete - Awaiting critical experiments for reviewer response

**Test Coverage:** 77% (552 tests, 98.9% pass rate)

**Key Achievement:** ACE demonstrates dramatic improvement (52-58%) with clear strategic behavior (99.8% concentration on collider parents)

---

## Project Organization & Current Workflow

### Project Structure (Clean & Organized - Updated Jan 26, 2026)

```
ACE/
├── README.md                       # Project overview
├── ace.sh                          # Unified CLI (all operations)
├── setup_env.sh                    # HPC environment setup
├── ace_experiments.py              # Main ACE implementation
├── baselines.py                    # Baseline methods
├── experiments/                    # Domain-specific SCMs
├── jobs/                           # SLURM job templates
│   ├── run_ace_main.sh             # ACE main experiment
│   ├── run_baselines.sh            # Baselines
│   ├── run_complex_scm.sh          # Complex SCM
│   ├── run_duffing.sh              # Duffing oscillators
│   ├── run_phillips.sh             # Phillips curve
│   ├── run_ablations.sh            # Ablation studies
│   ├── run_critical_experiments.sh # Critical experiments
│   └── workflows/                  # Workflow orchestration
│       ├── run_ace_only.sh         # ACE-only multi-seed script
│       ├── submit_ablations.sh     # Ablation submission
│       └── submit_ablations_scratch.sh
├── scripts/                        # Analysis & utility scripts
│   ├── analysis/                   # Analysis tools
│   │   ├── visualize.py            # Result visualization
│   │   ├── compare_methods.py      # Method comparison
│   │   ├── clamping_detector.py    # Verify clamping strategy
│   │   └── regime_analyzer.py      # Verify regime selection
│   ├── maintenance/                # Cleanup & maintenance
│   │   ├── cleanup_hpc.sh          # HPC disk cleanup
│   │   ├── cleanup_space.sh        # Space cleanup
│   │   ├── aggressive_cleanup.sh   # Aggressive cleanup
│   │   └── find_large_files.sh     # Find large files
│   ├── runners/                    # Local execution wrappers
│   │   ├── test_ace_quick.sh       # Quick ACE test
│   │   ├── run_ablations_local.sh  # Local ablations
│   │   └── run_critical_experiments.py
│   ├── compute_statistics.py       # Statistical analysis
│   ├── statistical_tests.py        # Statistical tests
│   ├── analyze_ablations.py        # Ablation analysis
│   └── test_failure_cases.py       # Failure case testing
├── tests/                          # Test suite (552 tests, 77% coverage)
├── paper/                          # LaTeX manuscript
├── results/                        # Experimental results
│   ├── ace/                        # ACE runs and summaries
│   ├── baselines/                  # Baseline runs
│   ├── complex_scm/                # Complex SCM runs
│   ├── duffing/                    # Duffing runs
│   ├── phillips/                   # Phillips runs
│   ├── logs/                       # Job logs (consolidated)
│   ├── summaries/                  # Result summaries
│   └── archive/                    # Archived results & docs
└── guidance_documents/             # Project documentation
    ├── guidance_doc.txt            # This file
    └── status_archive/             # Archived status files
```

### Current HPC Workflow (Fresh Start)

**Baselines:** COMPLETE (already run, N=5 each)
- Random, Round-Robin, Max-Variance, PPO: All complete with statistics

**ACE Experiments:** IN PROGRESS
Run with: `./jobs/workflows/run_ace_only.sh --seeds 5`

This submits 5 ACE jobs (one per seed):
1. **Seed 42** -> First reference run
2. **Seed 123** -> Second run
3. **Seed 456** -> Third run  
4. **Seed 789** -> Fourth run
5. **Seed 1011** -> Fifth run

All run on 5-node synthetic SCM benchmark.
Results combine for statistical validation (mean ± std, 95% CI).

**Output Naming Convention (Timestamp-based):**
- Format: `results/paper_YYYYMMDD_HHMMSS/`
- Example: `results/paper_20260121_143052/`
- Latest run is always last alphabetically (ls -t or ls | tail -1)
- Each run contains: `ace/`, `baselines/`, `complex_scm/`, `duffing/`, `phillips/`

**Finding Latest Run:**
```bash
ls -td results/paper_* | head -1 # Latest run directory
tail -f $(ls -t logs/ace_main_*.out | head -1) # Latest ACE log
```

**Verification Scripts:**
- `scripts/analysis/clamping_detector.py` -> Verify Line 661 claim (clamping)
- `scripts/analysis/regime_analyzer.py` -> Verify Line 714 claim (regime selection)
- `scripts/verify_claims.sh` -> Line 767 (early stopping), Line 485 (concentration)
- `scripts/analysis/compare_methods.py` -> Generate Table 1

### What's Complete [DONE]

1. [DONE] run_all.sh submits all necessary jobs
2. [DONE] All 5 job scripts exist and are executable
3. [DONE] Verification scripts exist (clamping, regime, compare)
4. [DONE] Extraction scripts exist (extract_ace.sh, extract_baselines.sh)
5. [DONE] All experiments are runnable
6. [DONE] Project structure is organized
7. [DONE] 77% test coverage validates components

### What's Complete for Paper Verification [DONE]

**Automated Post-Processing Workflow:**

1. [DONE] **Integrated Post-Processing Script**
 - Created: `scripts/process_all_results.sh`
 - Orchestrates:
 - Extract metrics from all jobs
 - Run verification (clamping, regime, concentration, early stopping)
 - Generate comparison tables
 - Create figures
 - Produce claim verification report
 - Single command: `./scripts/process_all_results.sh RESULTS_DIR`
 - Integrated into run_all.sh workflow

2. **LaTeX Table Generation**
 - `compare_methods.py` exists but need LaTeX output format
 - Need automated Table 1 generation ready for paper

3. **Claim-to-Evidence Mapping**
 - Document which paper claim is supported by which job/script
 - Automated verification all claims have supporting data

4. **Dependencies Documentation**
 - Need: `requirements.txt` for pip users
 - Need: `environment.yml` for conda users

**Important (Best Practices):**

5. **Results Documentation**
 - Need: `results/README.md` explaining output structure

6. **Post-Job Automation**
 - Workflow: run_all.sh completes -> auto-extract -> auto-verify -> auto-tables

7. **Figure Generation Automation**
 - Batch process all visualizations for paper

**Recommended Action Plan:**

```bash
# After run_all.sh completes:
./scripts/extract_ace.sh results/paper_TIMESTAMP/
./scripts/extract_baselines.sh results/paper_TIMESTAMP/
./scripts/verify_claims.sh results/paper_TIMESTAMP/
python scripts/analysis/compare_methods.py results/paper_TIMESTAMP/ --latex
python scripts/analysis/visualize.py results/paper_TIMESTAMP/*/ --paper-format
```

Or ideally:
```bash
./scripts/process_all_results.sh results/paper_TIMESTAMP/
```

This would generate:
- Table 1 (LaTeX format)
- All verification reports
- All paper figures
- Claim verification summary

**Complete improvements (19 total):**
1. Adaptive diversity threshold - allows strategic 60-75% concentration
2. Value novelty bonus - fixes zero-reward saturation (99% -> 40-60%)
3. Emergency retraining - prevents gradient death
4. Dynamic candidate reduction - 3-4x faster episodes (13.5m -> 3-5m)
5. Improved early stopping - triggers at 50-80 episodes
6. PPO bug fix - shape mismatch resolved
7. **Observational training restored** - prevents mechanism forgetting (every 3 steps)
8. Dedicated root learner - active every 3 episodes
9. Paper claims revised - 3 accuracy fixes (lines 609, 661, 714)
10. Verification tools - clamping detector, regime analyzer, extraction scripts

**Launch Commands:**
```bash
./pipeline_test.sh # Validate (30 min)
sbatch jobs/run_ace_main.sh # Launch DPO training (4-6 hours)
```

**Key Feature:** Dual observational training (step-level + episode-level) prevents catastrophic forgetting while enabling interventional learning.

**Previous updates:** See CHANGELOG below.

---

## Quick Start

```bash
# Full paper experiments (4-6 hours with improvements, was 12-15h)
./run_all.sh

# Quick validation (~30 minutes)
QUICK=true ./run_all.sh

# Individual components (with improvements)
python ace_experiments.py --episodes 200 --early_stopping --root_fitting
python baselines.py --all_with_ppo --episodes 100
python scripts/analysis/visualize.py results/run_*/
```

---

## Project Overview

ACE is a system for learning Structural Causal Models (SCMs) through autonomous experimentation. An agent policy proposes interventions (DO operations), executes them in a ground truth environment, and trains a student SCM to match the observed mechanisms.

### The Core Challenge: Collider Resolution

The ground truth SCM contains a **collider structure** that is particularly difficult to learn:

```
 X1 ──────────┬──────────-> X3 (COLLIDER)
 │ │ ↑
 └───-> X2 ───┘─────────────┘

 X4 ──────────-> X5
```

**Ground Truth Mechanisms:**
- X1 ~ N(0, 1) [exogenous root]
- X4 ~ N(2, 1) [exogenous root]
- X2 = 2.0 * X1 + 1.0 + ε
- X3 = 0.5 * X1 - X2 + sin(X2) + ε **[COLLIDER - 2 parents]**
- X5 = 0.2 * X4² + ε

**Why X3 is Hard:**
1. X3 has TWO parents (X1 and X2) - a collider
2. X1 and X2 are highly correlated: X2 = 2*X1 + 1
3. Under DO(X1=v), X2 still follows X2=2v+1, so they remain collinear
4. Only DO(X2=v) breaks this correlation, allowing independent variation
5. Without balanced interventions on BOTH X1 and X2, the learner cannot disentangle their effects

---

## Current Status (2026-01-21, 10:15 AM)

### Ready to Launch DPO Training [DONE]

**All improvements committed (7 git commits):**
- [DONE] Training fixes: Adaptive diversity, novelty bonus, emergency retrain, speedups
- [DONE] Observational training: RESTORED (step-level + dedicated root learner)
- [DONE] PPO bug: FIXED (shape mismatch resolved)
- [DONE] Paper revisions: 3 claims corrected for accuracy
- [DONE] Verification tools: clamping detector, regime analyzer, extraction scripts
- [DONE] Git: Clean, ready to push

**Experimental Status:**
- [DONE] Baselines: Complete (Round-Robin 1.99 best, Random 2.17, Max-Var 2.09)
- [DONE] Root learner: Validated (98.7% improvement)
- [DONE] Duffing & Phillips: Complete
- [PENDING] Complex SCM: Running (greedy_collider)
- [PENDING] ACE main: Ready to launch with all fixes

**Launch Commands:**
```bash
./pipeline_test.sh # Test fixes (30 min)
sbatch jobs/run_ace_main.sh # ACE training (4-6 hours)
```

**What Will Run:**
- 200 episodes (early stop expected at 40-80)
- Observational training every 3 steps (200 samples)
- Dedicated root learner every 3 episodes (1000 samples)
- All Jan 21 fixes active
- Expected runtime: 4-6 hours (vs previous 30-40h)

**After Training:**
```bash
./verify_claims.sh # Verify all paper claims
./extract_ace.sh # Extract metrics for tables
python scripts/analysis/compare_methods.py # Generate Table 1
```

### Paper Readiness
- [DONE] All 5 experiments analyzed and aligned
- [DONE] 3 paper claims revised for accuracy (lines 609, 661, 714)
- [PENDING] Need ACE results to fill Table 1
- [PENDING] Need verification of behavioral claims
- ETA to submission: 3-5 days after launch

### Paper Experiments Status
| Experiment | Paper Section | Status |
|------------|---------------|--------|
| Synthetic 5-node SCM | 3.4.1 | [DONE] `ace_experiments.py` |
| Complex 15-node SCM | 3.4.2 (NEW) | [DONE] `experiments/complex_scm.py` |
| Baselines (Random, Round-Robin, Max-Variance) | 3.8 | [DONE] `baselines.py` |
| PPO Comparison | Discussion | [DONE] `baselines.py` |
| Coupled Duffing Oscillators | 3.6 | [DONE] `experiments/duffing_oscillators.py` |
| US Phillips Curve (FRED) | 3.7 | [DONE] `experiments/phillips_curve.py` |

### Historical Context
Analysis of `run_20260103_141404` revealed original failure:
- LLM generated X1 for 100% of candidates
- DPO stuck at random chance (loss = 0.693)
- This was fixed via supervised pre-training and smart collapse breaker

---

## Current Solution Approach

### 1. Problem-First Prompt Restructuring
Old prompt buried loss info and ended with "DO" -> biased toward X1.
New prompt:
- STARTS with "PROBLEM: Node losses - X3=1.82, X2=0.01..."
- Explicitly says "To fix X3, intervene on parents X1, X2"
- Warns "X1 is over-sampled. AVOID X1."
- Includes few-shot reasoning examples
- Ends with "The highest loss node is" to prompt reasoning

### 2. Supervised Pre-training Before DPO
- `supervised_pretrain_llm()` runs 100 steps BEFORE DPO
- Trains on teacher-generated interventions (loss-aware, balanced)
- Gives LLM proper starting point instead of random X1 fixation
- Periodic re-training every 50 episodes to combat drift

### 3. Smart Collapse Breaker
- When all candidates target collapsed node and collapse detected:
- PRIORITIZE parents of high-loss colliders (X2 for failing X3)
- Don't randomly inject X3/X4/X5 which don't help

### 4. Comprehensive DPO Logging
New `DPOLogger` class tracks:
- Loss values and trends
- Preference margins (policy_win - policy_lose)
- Sigmoid inputs (positive = learning, near-zero = stuck)
- KL divergence from reference policy
- Winner/loser target distribution
- Periodic health check reports with warnings

### 5. Gradient Monitoring
- Logs gradient norms every 20 episodes
- Warns if near-zero (DPO not training)

---

## Output Files Per Run

Each run generates:
1. `scm_graph.png` - **NEW** Visualization of the SCM structure with node types
2. `mechanism_contrast.png` - Learned vs ground truth mechanisms
3. `training_curves.png` - DPO loss and reward over time
4. `strategy_analysis.png` - Target preference and value distribution
5. `metrics.csv` - Per-step metrics (target, value, reward, score)
6. `node_losses.csv` - Per-step per-node mechanism losses
7. `dpo_training.csv` - **NEW** DPO training diagnostics
8. `experiment.log` - Full execution log

---

## Results Documentation Protocol

**CRITICAL:** Whenever an experimental result supports a paper claim, document it immediately in `results/RESULTS_LOG.md`.

### Why Document Results?

1. **Audit Trail:** Clear path from raw data -> evidence -> paper claims
2. **Prevent Loss:** Don't lose important findings in log files
3. **Track Claims:** Know which claims are supported vs aspirational
4. **Review Readiness:** Easy to verify all claims before submission

### What to Document

Document EVERY experimental run that:
- [DONE] Supports a paper claim
- [WARNING] Partially supports a paper claim
- [NO] Contradicts a paper claim
- [HYPOTHESIS] Tests a specific hypothesis

### When to Document

**IMMEDIATELY** after:
- Completing a full experimental run
- Extracting metrics from logs
- Identifying a key finding
- Discovering an unexpected result

### How to Document

1. **Add entry to `results/RESULTS_LOG.md`** with:
 - Date and run ID
 - Key metrics and findings
 - Which paper lines/claims it supports
 - Path to evidence files
 - Any caveats or issues

2. **For major findings, create `results/claim_evidence/[claim].md`**
 - Detailed evidence for specific claim
 - Multiple runs supporting same claim
 - Statistical analysis if available

3. **Save publication-ready figures to `results/figures/`**
 - Include metadata about source run
 - Use consistent naming convention

### Example Workflow

```bash
# 1. Run experiment
sbatch jobs/run_ace_main.sh

# 2. Wait for completion
tail -f logs/ace_*.out

# 3. Extract key metrics
grep "Final mechanism losses:" results/paper_*/ace/experiment.log

# 4. IMMEDIATELY document in results/RESULTS_LOG.md
code results/RESULTS_LOG.md
# Add new entry with date, metrics, paper claims supported

# 5. If this supports a paper table, note it
# Example: "Fills Table 1, line 428-437"

# 6. Generate figures if needed
python scripts/analysis/visualize.py results/paper_*/ace/
cp results/paper_*/ace/learning_curves.png results/figures/

# 7. Link to paper
# In RESULTS_LOG.md, note: "Supports line 439: superior collider identification"
```

### Before Writing Paper Claims

**DO:**
- [DONE] Check `results/RESULTS_LOG.md` for supporting evidence
- [DONE] Use actual numbers from documented runs
- [DONE] Reference specific evidence files
- [DONE] Note any caveats or limitations

**DON'T:**
- [NO] Write claims before having results
- [NO] Use placeholder values
- [NO] Make aspirational claims without marking them
- [NO] Forget to document negative results

### Before Paper Submission

**Checklist:**
- [ ] Every paper claim has entry in `results/RESULTS_LOG.md`
- [ ] All tables filled with real numbers (no placeholders)
- [ ] All figures generated and saved to `results/figures/`
- [ ] Evidence files accessible and organized
- [ ] Summary dashboard in RESULTS_LOG.md up to date

See `results/README.md` for detailed templates and examples.

---

## Testing Infrastructure

### Comprehensive Test Coverage (January 21, 2026)

**Status:** 60% coverage achieved, continuing toward 90% goal 
**Tests:** 239 passing (99% pass rate) 
**Documentation:** See `TESTING.md` for complete guide 
**Framework:** Modern pytest with statistical and property-based testing

#### Testing Strategy

**Test Categories:**
1. **Unit Tests** - Fast (<100ms), isolated component tests
2. **Integration Tests** - Multi-component workflow tests
3. **Statistical Tests** - Probabilistic assertions with tolerances
4. **Regression Tests** - Performance baseline tracking
5. **Property Tests** - Hypothesis-based invariant testing

**Coverage Targets:**
- Core SCM classes: 95%
- Policy implementations: 90%
- Baseline policies: 90%
- Training loops: 85%
- Experiments: 80%
- Visualization: 70%

#### Key Testing Principles

1. **Behavior over Implementation**: Test external expectations, not internal details
2. **Statistical Assertions**: Use `pytest.approx` for probabilistic components
3. **Reproducibility**: All tests use fixed seeds (`seed_everything` fixture)
4. **Fast Feedback**: Unit tests complete in <1s, integration in <30s
5. **Regression Prevention**: Track baselines for all key metrics

#### Test Framework

**Dependencies:**
```bash
pytest>=8.0.0
pytest-cov>=5.0.0
pytest-mock>=3.12.0
pytest-timeout>=2.2.0
pytest-xdist>=3.5.0 # Parallel execution
hypothesis>=6.100.0 # Property-based testing
```

**Running Tests:**
```bash
# All unit tests (fast)
pytest -m unit

# With coverage
pytest -m unit --cov=. --cov-report=html

# Integration tests
pytest -m integration

# All except slow
pytest -m "not slow"

# Parallel execution
pytest -n auto
```

#### Test Structure

```
tests/
├── conftest.py # Shared fixtures
├── pytest.ini # Configuration
├── test_ground_truth_scm.py # Core SCM tests
├── test_student_scm.py # Student model tests
├── test_llm_policy.py # LLM policy tests
├── test_dpo_trainer.py # DPO training tests
├── test_reward_functions.py # Reward computation tests
├── baselines/ # Baseline policy tests
├── integration/ # E2E workflow tests
├── statistical/ # Property tests
├── regression/ # Performance tracking
└── experiments/ # Experiment-specific tests
```

#### Key Fixtures

```python
@pytest.fixture
def seed_everything():
 """Set all random seeds for reproducibility."""

@pytest.fixture
def ground_truth_scm():
 """GroundTruthSCM instance."""

@pytest.fixture
def student_scm():
 """Untrained StudentSCM instance."""

@pytest.fixture
def mock_llm():
 """Mock LLM for GPU-free testing."""

@pytest.fixture
def test_output_dir():
 """Temporary directory for test outputs."""
```

#### Test Markers

```python
@pytest.mark.unit # Fast, isolated tests
@pytest.mark.integration # Multi-component tests
@pytest.mark.slow # End-to-end tests (>30s)
@pytest.mark.statistical # Probabilistic assertions
@pytest.mark.regression # Performance tracking
@pytest.mark.requires_gpu # GPU-dependent tests
@pytest.mark.requires_hf # HuggingFace models required
```

#### Example Test

```python
@pytest.mark.unit
@pytest.mark.statistical
def test_x3_collider_mechanism(scm, seed_everything):
 """Test X3 = 0.5*X1 - X2 + sin(X2) mechanism."""
 seed_everything(42)
 n = 10000

 # Generate observational data
 data = scm.generate(n)

 # Compute expected X3
 expected = 0.5 * data['X1'] - data['X2'] + torch.sin(data['X2'])

 # Assert match (allowing for noise std=0.1)
 actual = data['X3']
 residuals = (actual - expected).abs()

 # Should be centered near 0 with std ≈ 0.1
 assert residuals.mean() == pytest.approx(0.0, abs=0.05)
 assert residuals.std() == pytest.approx(0.1, abs=0.05)
```

#### CI/CD Integration

**GitHub Actions** (`.github/workflows/test.yml`):
- Run on every push/PR
- Test on Python 3.9, 3.10, 3.11
- Upload coverage to Codecov
- Block merge if coverage drops

**Pre-commit Hooks**:
- Run unit tests before commit
- Ensure code quality

#### Implementation Timeline

| Week | Phase | Deliverables |
|------|-------|-------------|
| 1 | Infrastructure + Core | Framework, SCM tests |
| 2-3 | Policy Components | LLM, DPO, reward tests |
| 4 | Integration | E2E workflow tests |
| 5 | Statistical + Regression | Property, baseline tests |
| 6 | Experiments | Domain-specific tests |
| 7 | CI/CD | GitHub Actions, docs |

**Total Effort:** 7 weeks (1 developer full-time)

#### Maintenance

**Ongoing:**
- Update tests with code changes
- Update regression baselines after verified improvements
- Monitor coverage in CI/CD
- Hunt and fix flaky tests
- Track test suite performance

**Quarterly:**
- Review coverage reports
- Identify undertested components
- Update strategy based on bug patterns
- Refactor for maintainability

#### Success Metrics

| Milestone | Unit Cov | Integration Cov | Overall Cov |
|-----------|----------|----------------|-------------|
| Week 2 | 50% | 0% | 40% |
| Week 4 | 80% | 50% | 70% |
| Week 6 | 90% | 80% | 85% |
| Complete | 95% | 90% | 90%+ |

**Quality Targets:**
- All tests pass on Python 3.9-3.11
- Test suite completes in <5 minutes
- >99% pass rate (no flaky tests)
- Critical components >90% coverage

---

## Project Scripts

### Main Experiment: `ace_experiments.py`
The core ACE framework with LLM-based policy and DPO training.

### Baselines: `baselines.py`
Implements four baseline intervention strategies for comparison:

1. **Random Policy (Lower Bound)**
 - Uniformly samples target node and intervention value
 - Represents unguided, passive exploration

2. **Round-Robin (Systematic Heuristic)**
 - Cycles through nodes in fixed topological order
 - Tests if adaptive, non-uniform sampling is necessary

3. **Max-Variance (Uncertainty Sampling)**
 - Greedy active learning using MC Dropout
 - Selects interventions maximizing predictive variance
 - Represents standard "greedy" optimal experimental design

4. **PPO (Proximal Policy Optimization)**
 - Actor-Critic RL baseline for fair comparison with DPO
 - Same reward signal: Information Gain + coverage bonuses
 - Same replay buffer, observational training interval
 - Uses GAE for advantage estimation, clipped surrogate objective
 - Tests paper's claim that DPO outperforms value-based RL

```bash
# Run individual baseline
python baselines.py --baseline random --episodes 100
python baselines.py --baseline round_robin --episodes 100
python baselines.py --baseline max_variance --episodes 100
python baselines.py --baseline ppo --episodes 100

# Run all baselines (excluding PPO - faster)
python baselines.py --all --episodes 100

# Run all baselines including PPO
python baselines.py --all_with_ppo --episodes 100
```

### Visualization: `visualize.py`
```bash
python scripts/analysis/visualize.py run_YYYYMMDD_HHMMSS/ # Success verification dashboard
```

### Additional Experiments: `experiments/`
```bash
# Complex 15-Node SCM (Hard benchmark to show strategy advantages)
python -m experiments.complex_scm --policy random --episodes 200
python -m experiments.complex_scm --policy smart_random --episodes 200
python -m experiments.complex_scm --policy greedy_collider --episodes 200

# Duffing Oscillators (Physics - ODE-based causal discovery)
python -m experiments.duffing_oscillators --episodes 100

# Phillips Curve (Economics - FRED data retrospective learning)
python -m experiments.phillips_curve --episodes 50
```

**Why Complex SCM?**
The simple 5-node SCM proved too easy - even random sampling learns all mechanisms.
The 15-node SCM is designed to be HARD:
- 15 nodes (3x larger)
- 5 colliders including nested collider (N1 depends on L1 which is itself a collider)
- 5 hierarchical layers (vs 2)
- Mix of linear, polynomial, trig, interaction terms
- Higher noise (0.2 vs 0.1)
- Random sampling spreads samples too thin across 15 nodes

This should reveal advantages of strategic intervention policies.

### Full Pipeline: `run_all.sh` (HPC/SLURM)
```bash
sbatch run_all.sh # Full paper experiments
QUICK=true sbatch run_all.sh # Quick validation
ACE_EPISODES=200 sbatch run_all.sh # Custom episodes
squeue -u $USER # Monitor job
tail -f logs/paper_*.out # View progress
```

---

## Key CLI Arguments

### ACE Main Script
```bash
python ace_experiments.py \
 --episodes 100 \
 --pretrain_steps 100 \ # Supervised pre-training before DPO
 --pretrain_interval 50 \ # Re-train every N episodes
 --smart_breaker \ # Prioritize collider parents in collapse breaker
 --obs_train_interval 5 \ # Observational training every N steps (0=disabled)
 --obs_train_samples 100 \ # Samples per observational injection
 --obs_train_epochs 50 \ # Training epochs for observational data
 --debug_parsing # Enable parse debug logging
```

### Baselines Script
```bash
python baselines.py \
 --baseline random \ # Or: round_robin, max_variance, ppo
 --all \ # Run all baselines (excluding PPO)
 --all_with_ppo \ # Run all baselines including PPO
 --episodes 50 \ # Number of episodes
 --steps 25 \ # Steps per episode
 --obs_train_interval 5 \ # Observational training interval
 --ppo_lr 3e-4 \ # PPO learning rate
 --ppo_epochs 4 \ # PPO epochs per update
 --ppo_clip 0.2 \ # PPO clip epsilon
 --output results # Output directory
```

### Run All Script (Paper Experiments)
```bash
python run_all.py \
 --quick \ # Fast validation (~5 min)
 --full \ # Full paper experiments (~2-4 hours)
 --ace_episodes 500 \ # Override ACE episodes
 --baseline_episodes 100 \ # Override baseline episodes
 --use_custom \ # Use custom transformer (faster)
 --skip_ppo \ # Skip PPO baseline
 --timeout 14400 # Timeout per experiment (seconds)
```

---

## Success Criteria

### Primary (All mechanisms learned)
1. **X3 Loss < 0.5** - The collider mechanism is learned [DONE] (achieved ~0.07)
2. **X2 Loss < 1.0** - The linear mechanism is preserved (requires obs training)
3. **X5 Loss < 0.5** - The quadratic mechanism is learned [DONE] (achieved ~0.18)
4. **X1, X4 Loss < 1.0** - Root distributions captured

### Secondary (Training health)
5. **DPO Learning** - Loss should decrease from 0.693 toward 0
6. **Preference Margin Positive** - Policy should prefer winners >50% of time
7. **Intervention Diversity** - No single node >70% of interventions

### Paper Validation
8. **ACE > Random** - Lower final loss than random baseline
9. **ACE > Round-Robin** - Lower final loss than systematic baseline
10. **ACE > Max-Variance** - Lower final loss than greedy active learning
11. **ACE > PPO** - Lower final loss than value-based RL (validates paper claim)

---

## Debugging Checklist

### If X3 (collider) isn't learning:
1. Check `metrics.csv` - Is X2 getting interventions?
2. Check `value_diversity.csv` - Are X2 values diverse (not stuck at single value)?
3. Check `dpo_training.csv` - Is preference_margin positive?
4. Check `mechanism_contrast.png` - Are X2->X3 and X1->X3 slices accurate?

### If X2 mechanism is forgotten (high loss):
1. Verify `--obs_train_interval` is enabled (default: 3)
2. Check intervention distribution - if >90% X2, increase observational training
3. Check `mechanism_contrast.png` - Is X1->X2 line flat (untrained network)?

### If root nodes (X1, X4) aren't learning:
1. Enable `--root_fitting` for explicit root distribution fitting
2. Increase observational training: `--obs_train_interval 2`
3. Check that root_fit logs appear every 5 episodes
4. Verify interventions aren't overshadowing observational data

### If DPO isn't learning:
1. Check gradient norms in log - Are they non-zero?
2. Check `dpo_training.csv` - Is loss stuck at 0.693?
3. Verify supervised pre-training ran (`--pretrain_steps 100`)

### General:
4. Check `scm_graph.png` - Verify graph structure is correct
5. Run `python scripts/analysis/visualize.py <run_dir>/` for comprehensive dashboard

---

# CHANGELOG

## 2026-01-15: Periodic Observational Training to Prevent Mechanism Forgetting

### Problem: Catastrophic Forgetting of X2 Mechanism
Analysis of `run_20260113_060752` and `run_20260114_095334` revealed:
- X3 collider was learned successfully (loss ~0.07) [x]
- X2 mechanism was **catastrophically forgotten** (loss ~22) [x]
- Intervention distribution was 98.9% X2, only 1.1% X1

**Root Cause:**
When `DO(X2=v)` is applied, X2 is set directly to value `v`. This means:
1. The student's X2 mechanism `f(X1) -> X2` is **never called**
2. The X1->X2 relationship `X2 = 2·X1 + 1` receives no gradient updates
3. After 1500+ X2 interventions, the X2 mechanism network outputs garbage

### Solution: Periodic Observational Training
Every N steps, inject observational data (no intervention) to preserve all mechanisms:

```python
# New CLI arguments:
--obs_train_interval 5 # Train on observational data every 5 steps
--obs_train_samples 100 # Number of samples per injection
--obs_train_epochs 50 # Training epochs for observational data
```

**Implementation:**
- After each `learner.train_step(intervention_data)`, check if `step % obs_train_interval == 0`
- If so, generate observational samples: `M_star.generate(n_samples, interventions=None)`
- Train the student on this data, which includes the natural X2 mechanism

### Expected Outcome
- X2 mechanism should now be preserved (loss < 1.0)
- X3 collider should still be learned (loss < 0.5)
- Both mechanisms coexist without catastrophic forgetting

---

## 2026-01-13: Value-Aware Collapse Breaking for Collider Disentanglement

### Problem: Single-Value Trap (X2=c)
Analysis of `run_20260112_011254` revealed a new failure mode:
- The agent correctly shifted from X1 to X2.
- However, it collapsed to a **single static value** `DO(X2 = 1.5)`.
- **Consequence:** 
 - The learner observed a 1D slice of the X3 mechanism ($X3 = f(X1, 1.5)$).
 - This slice is easily fit by a linear model, resulting in **Zero Training Loss**.
 - The validation set (where X2 varies) showed **High Error**, but the reward signal (delta loss) was zero because the agent kept repeating the same "perfect" action.
 - The standard Collapse Breaker failed because it injected X1 (correlated) or X4 (irrelevant), never forcing X2 to vary.

### Solution: Value-Aware Collapse Breaker
We implemented a principled extension to the Smart Breaker:
1. **Context Detection:** Identifies if the collapsed node is a **Parent of a Collider** (e.g., X2 is parent of X3).
2. **Constraint:** Recognizes that for collider parents, *existence* of intervention isn't enough; *variance* is required.
3. **Action:** Injects an intervention for the **SAME NODE** (X2) but with a **RADICALLY DIFFERENT VALUE** (e.g., flipping sign or moving to bounds).
4. **Mechanism:**
 - Calculates mean of recent values (e.g., 1.5).
 - Samples `new_val` from a disjoint range (e.g., [-5, 0]).
 - This forces the learner to predict on a new support region, exposing the error in its simplified model and generating a strong gradient signal.

### Expected Outcome
- The logs should show `[Smart Breaker] Injected 'DO X2 = -3.5' (value_diversity_for_collider_parent...)`.
- X3 Loss should drop significantly as the learner sees X2 vary independently of X1.
- DPO should pick up this signal (via the "Epistemic Boost") and learn to vary X2 autonomously.

---

## 2026-01-11: Comprehensive Logging and SCM Visualization

### Changes
1. **SCM Graph Visualization** (`scripts/analysis/visualize.py`)
 - Generates `scm_graph.png` for each run
 - Shows node types: root (green), intermediate (blue), collider (red), leaf (purple)
 - Displays mechanism equations and final losses
 - Uses hierarchical layout for clear causal structure

2. **DPO Training Logger** (`DPOLogger`)
 - Tracks loss, preference margins, sigmoid inputs, KL divergence
 - Periodic health reports every 100 steps
 - Warns when DPO appears stuck (loss ~0.693, negative preference margin)
 - Saves to `dpo_training.csv` for post-hoc analysis

3. **Enhanced DPO Loss Function**
 - Now logs all components for diagnosis
 - Extracts winner/loser targets for tracking

---

## 2026-01-11: LLM Policy Attention Failure - Direct Intervention

### Problem: LLM Completely Ignores Prompt
- Total Policy Collapse: 100% of candidates targeted X1
- DPO stuck at 0.693 (random chance)
- X3 loss remained at 1.8-2.5 while X2 = 0.01

### Solution
1. Problem-first prompt restructuring
2. Supervised pre-training phase (100 steps before DPO)
3. Smart collapse breaker (prioritizes collider parents)
4. Gradient monitoring every 20 episodes
5. LLM fallback on parse failure

---

## 2026-01-08: Policy Input Augmentation for Collider Resolution

### Problem
Agent was "blind" - only saw weights, not losses. Couldn't learn "if X3 failing, intervene on X2".

### Solution
1. Augmented StateEncoder with per-node validation loss
2. LLM prompt includes loss values
3. Reward clipping to -2.0 (reduced punishment for exploration)

---

## 2026-01-03: Fast Adaptation Phase

### Problem
Reward misattribution due to delayed learning updates.

### Solution
Added fast adaptation phase that trains on NEW data before replay consolidation.

---

## 2026-01-20: Comprehensive Training Efficiency Overhaul

### Problems Identified from January 19, 2026 HPC Run Analysis
Based on detailed artifact analysis of `run_20260119_123852`:
1. **Training Saturation:** 89.3% of steps (3,358/3,760) produced zero reward
2. **Root Node Failure:** X1 showed NO learning (0.879->0.879), X4 got worse (1.506->1.564)
3. **Policy Collapse:** LLM generated X2 for 99.1% of candidates
4. **KL Divergence Explosion:** Reference policy diverged from 0 -> -2,300

### Solutions Implemented

#### 1. Early Stopping System
**Class:** `EarlyStopping` (lines 1413-1467)
- Detects loss plateaus (no improvement for N episodes)
- Detects training saturation (>85% zero-reward steps)
- **Impact:** Reduces runtime from 9h 11m to 1-2h (80% savings)

**New Arguments:**
```bash
--early_stopping # Enable early stopping
--early_stop_patience 20 # Episodes to wait before stopping
--early_stop_min_delta 0.01 # Minimum improvement threshold
--zero_reward_threshold 0.85 # Stop if 85% steps have zero reward
```

#### 2. Root Node Learning Enhancements
**Problem:** Interventions DO(X1=v) override natural distribution N(0,1)
**Solution:** 3x increase in observational training + explicit root fitting

**Updated Defaults:**
```bash
--obs_train_interval 3 # Was 5 (67% more frequent)
--obs_train_samples 200 # Was 100 (2x samples)
--obs_train_epochs 100 # Was 50 (2x epochs)
```

**New Function:** `fit_root_distributions()` (lines 1470-1547)
- Explicitly fits root distributions using pure observational data
- Trains mu and sigma parameters directly
- Called every 5 episodes

**New Arguments:**
```bash
--root_fitting # Enable root-specific fitting
--root_fit_interval 5 # Fit every N episodes
--root_fit_samples 500 # Samples for root fitting
--root_fit_epochs 100 # Epochs for root fitting
```

**Expected Impact:**
- X1 loss: 0.879 -> <0.3 (66% reduction)
- X4 loss: 0.942 -> <0.3 (68% reduction)

#### 3. Multi-Objective Diversity Rewards
**Problem:** Policy collapsed to 69.4% X2, requiring constant hard-cap enforcement
**Solution:** Smooth diversity penalties + coverage bonuses

**New Functions:**
- `compute_diversity_penalty()` (lines 1549-1578): Penalizes concentration >50%
- `compute_coverage_bonus()` (lines 1581-1601): Rewards exploring all nodes

**Updated Defaults:**
```bash
--undersampled_bonus 200.0 # Was 100.0 (2x stronger)
```

**New Arguments:**
```bash
--diversity_reward_weight 0.3 # Weight for diversity in multi-objective reward
--max_concentration 0.5 # Maximum 50% on any single node
--concentration_penalty 200.0 # Penalty weight for exceeding max
```

**Reward Formula (Updated):**
```python
base_score = reward + cov_bonus + ... - leaf_pen - collapse_pen
diversity_score = diversity_penalty + coverage_bonus
final_score = base_score + diversity_weight * diversity_score
```

**Expected Impact:**
- X2 concentration: 69.4% -> <50%
- X4 interventions: 0.9% -> >15% (16x increase)
- X5 interventions: 0.0% -> >10%

#### 4. Reference Policy Stability
**Problem:** KL divergence from 0 -> -2,300 (policy abandoned supervised initialization)
**Solution:** Periodic reference policy updates

**New Arguments:**
```bash
--update_reference_interval 25 # Update reference every N episodes
```

**Implementation:** Updates reference policy independently of supervised re-training,
keeping KL divergence bounded and maintaining connection to initialization.

### Usage

**Recommended Command (All Improvements):**
```bash
python ace_experiments.py \
 --episodes 200 \
 --early_stopping \
 --early_stop_patience 20 \
 --obs_train_interval 3 \
 --obs_train_samples 200 \
 --obs_train_epochs 100 \
 --root_fitting \
 --root_fit_interval 5 \
 --undersampled_bonus 200.0 \
 --diversity_reward_weight 0.3 \
 --max_concentration 0.5 \
 --update_reference_interval 25 \
 --pretrain_steps 200 \
 --pretrain_interval 25 \
 --smart_breaker \
 --output results/ace_improved
```

**Quick Test (10 episodes):**
```bash
python ace_experiments.py \
 --episodes 10 \
 --early_stopping \
 --root_fitting \
 --output results/test
```

### Expected Overall Performance

| Metric | Before (Jan 19) | After (Jan 20) | Improvement |
|--------|-----------------|----------------|-------------|
| Runtime | 9h 11m | 1-2h | 80% ↓ |
| X1 Loss | 0.879 | <0.3 | 66% ↓ |
| X4 Loss | 0.942 | <0.3 | 68% ↓ |
| X2 Concentration | 69.4% | <50% | 28% ↓ |
| Zero-reward Steps | 89.3% | <50% | 44% ↓ |
| Total Loss | 1.92 | <1.0 | 48% ↓ |

### Backwards Compatibility
All improvements are opt-in via flags. Existing scripts work unchanged.
New defaults (obs_train_interval=3, undersampled_bonus=200) improve performance automatically.


## Current Experimental Results (January 25, 2026)

### Baseline Performance (COMPLETE)

**5-Node Synthetic Benchmark (N=5 runs each):**

| Method | Final Loss | Std | 95% CI | Episodes |
|--------|------------|-----|--------|----------|
| Random | 2.18 | 0.06 | [2.13, 2.24] | 100 |
| Round-Robin | 2.15 | 0.08 | [2.08, 2.21] | 100 |
| Max-Variance | 2.05 | 0.12 | [1.94, 2.15] | 100 |
| PPO | 2.11 | 0.13 | [2.00, 2.23] | 100 |

**Best Baseline:** Max-Variance at 2.05 ± 0.12

**Statistical Validation:**
- Paired t-tests confirm Max-Var significantly better than Random (p=0.021)
- 13% improvement: Max-Var vs Random (2.05 vs 2.18)
- PPO competitive but higher variance (std=0.13)

**Duffing Oscillator (N=5 runs):**
- Final loss: 0.042 ± 0.036 (95% CI: [0.011, 0.073])
- All runs successfully recovered chain structure
- Validates spurious correlation breaking strategy

**Complex 15-Node SCM (N=6 runs):**
- Random: 4.65 ± 0.21 total MSE, 0.32 ± 0.04 collider MSE (N=4)
- Greedy collider: 4.49 total, 0.29 collider (N=1)
- 9% collider improvement with targeting

### ACE Performance (PENDING)

**Status:** Awaiting fresh multi-seed runs

**Target Performance:**
- Final loss: ≤ 2.11 (match PPO or better)
- Ideal: ≤ 2.05 (match Max-Variance)
- Episodes: < 100 (ideally 40-60 with early stopping)
- Episode reduction: 40-60% vs baselines

**To Submit:**
```bash
cd ~/ACE
source setup_env.sh
./jobs/workflows/run_ace_only.sh --seeds 5
```

**Expected Timeline:** 5-10 hours on HPC

### Paper Population Status

**Complete Sections (Real Data):**
- Synthetic benchmark baselines (all 4 methods with statistics)
- Duffing oscillator results (N=5)
- Complex SCM strategy comparison (N=6)
- Statistical validation (CIs, p-values)

**Pending (Awaiting ACE):**
- ACE vs baseline comparison
- Episode reduction validation
- Early stopping demonstration
- Final performance claims

**Paper Completeness:** 80% (baselines complete, ACE pending)

## Commands for Fresh ACE Run

### HPC Setup (One-Time)
```bash
cd ~/ACE
git pull origin main
source setup_env.sh
```

### Submit ACE Experiments
```bash
# 5-seed multi-seed validation
./run_ace_only.sh --seeds 5

# Monitor
squeue -u $USER
tail -f results/logs/ace_seed42_*.out

# After completion
python scripts/compute_statistics.py results/ace_multi_seed_*/ ace
```

### Analysis After ACE Completes
```bash
# Extract ACE statistics
python scripts/compute_statistics.py results/ace_multi_seed_*/ ace

# Compare to baselines (already have baseline stats)
# Expected ACE: X.XX ± Y.YY (ZZ episodes)
# Compare to Max-Var: 2.05 ± 0.12 (100 episodes)

# If ACE ≤ 2.11 and episodes < 100:
# -> Populate paper with results
# -> Paper 100% complete!
```

## Success Criteria

**ACE Must Achieve:**
1. Final loss ≤ 2.11 (match or beat PPO)
2. Episodes < 100 (demonstrate efficiency)
3. All nodes converged (no high-loss outliers)
4. Stable across seeds (std < 0.15)

**If achieved:** Paper claims validated, ready for submission!

