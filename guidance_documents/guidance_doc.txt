Here is the method generally described here. We need to revise because based on the results we got from our last run, we appear to simply not
be learning the functional associations between nodes in the SCM, which is the entire point of what we are after here. We are struggling with colliders and need to resolve this.

However, we have a massive issue where the parsing is not operating correctly. Note the result of our last experimental run. We need to address this before we can continue work on this method:
  Step 11: 'DO X2 = 3.4491' (Reward: 0.10, Cov: 6.50, Score: 6.60, RecentTop: X1@52%)
  Step 12: 'DO X2 = 4.0311' (Reward: 0.10, Cov: 8.36, Score: 8.46, RecentTop: X1@52%)
  Step 13: 'DO X2 = -1.2150' (Reward: 0.10, Cov: 8.52, Score: 8.62, RecentTop: X1@52%)
  Step 14: 'DO X1 = 4.7897' (Reward: 0.10, Cov: 7.05, Score: 7.15, RecentTop: X1@52%)
  Step 15: 'DO X1 = 2.0501' (Reward: 0.10, Cov: 7.26, Score: 7.36, RecentTop: X1@52%)
  Step 16: 'DO X2 = -2.2246' (Reward: 0.10, Cov: 7.49, Score: 7.59, RecentTop: X1@52%)
  Step 17: 'DO X2 = -4.6019' (Reward: 0.10, Cov: 3.00, Score: 3.10, RecentTop: X1@52%)
  Step 18: 'DO X2 = -1.2866' (Reward: 0.10, Cov: 1.25, Score: 1.35, RecentTop: X1@52%)
  Step 19: 'DO X2 = 1.0068' (Reward: 0.10, Cov: 1.00, Score: 1.10, RecentTop: X1@52%)
  Step 20: 'DO X1 = -0.1455' (Reward: 0.10, Cov: 0.94, Score: 1.04, RecentTop: X1@52%)
--- Running Final Evaluation ---
Final eval produced 0 parsed commands; falling back to last 500 training interventions for plots.
Run ended at: 2025-12-29T10:35:24
Run duration: 1:40:37.023190
Training parse stats: candidates_parsed=61/8492 (0.7%), steps_with_any_valid_candidate=34/2183 (1.6%), teacher_fallback_steps=2094/2183 (95.9%)
Experiment Complete. Results saved to experiment_results/run_20251229_085447



# ACE: Adversarial Causal Experimentalist

**A Framework for Automated Causal Discovery via DSL-Mediated Self-Play and Direct Preference Optimization (DPO).**

## Overview

**ACE** (Adversarial Self-Alignment for Causal Experimentalism) is a research framework that reformulates causal discovery not as a static curve-fitting problem, but as an interactive, sequential decision-making game.

The goal is to train an AI agent (the **Experimentalist**) to autonomously design experiments that efficiently uncover the hidden mechanisms of a ground-truth system (the **Environment**). By treating the "Learner's" ignorance as an adversary, the agent uses **Direct Preference Optimization (DPO)** to align itself with the objective of maximizing "Scientific Surprise" (Information Gain).

## Key Features

* **Interactive Causal Discovery:** The agent actively queries the environment using interventions ($do(X=x)$), rather than learning from passive observational data.
* **Episodic Discovery Protocol:** Training is organized into "Episodes" where a fresh Learner attempts to solve the system from scratch, forcing the Agent to learn generalizable strategies rather than memorizing a single solution.
* **Dual-Policy Architecture:** Supports both custom scratch-trained Transformers and pretrained LLM Adapters (e.g., **Qwen-2.5**) to guide experimentation.
* **Rigorous DSL:** All interventions are grounded in a Domain Specific Language (DSL) to ensure valid, physically realizable experiments (e.g., `DO X1 = 2.5`).
* **Teacher Injection:** A bootstrapping mechanism that injects valid "Teacher" commands during early training to overcome the cold-start problem and prevent reward hacking.

## System Architecture

The framework consists of three primary interacting components:

1.  **The Environment ($M^*$):** A ground-truth Structural Causal Model (SCM) that generates data. It supports complex non-linear mechanisms (e.g., Sine waves, quadratic functions) to rigorously test discovery capabilities.
2.  **The Learner ($M_\theta$):** A learnable SCM parameterized by Neural Networks (MLPs). It attempts to approximate the Environment's mechanisms based on data gathered by the Agent.
3.  **The Experimentalist ($\pi_\phi$):** The policy (Agent) that observes the Learner's current state (weights, uncertainty) and proposes the next experiment to run.
